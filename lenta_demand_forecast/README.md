# lenta_hackathon

Были изучены 4 файла с данными: pr_df, st_df, sales_submission, sales_df_train.  

**pr_df - данные по товарной иерархии.**  
Уникальных товаров - 2050. Товары делятся на 9 групп, далее делятся на 43 категории и 170 подкатегорий.  

В столбце pr_uom_id (маркер, обозначающий продаётся товар на вес или в шт) данные разделились:
1279 товаров в группе 1;
771 товар в группе 17.  

**st_df - данные по магазинам.**  
В данных 12 магазинов, они находятся в 7 городах. Дивизионов - 6. Форматов магазинов - 3 (1, 2, 4). Типов локации/окружения - 3 (1, 2, 3). Типов размера магазина - 6 (8, 12, 19, 20, 28, 32).
Активных магазинов - 10. По неактивным магазинам прогноз строить не нужно.

**sales_submission - данные для прогноза.**  
В файл входят столбцы с id магазина, id товара, датой и нулевым значением целевой переменной. Спрогнозировать спрос нужно на 14 дней за период с 19 июля по 01 августа 2023г. Всего магазинов в файле - 8. Именно по ним и будет строиться прогноз.

**sales_df_train - данные по продажам.**  
В данных по продажам содержится информация о продажах 1973 товара в 12 магазинах. Количество проданных товаров варьируется от 2 и 6 до 1074. Магазины с небольшим числом проданных товаров, возможно, закрыты или недавно открыты, так как для них указана информация только за 6 - 31 дней, всего таких магазинов - 3. Ассортимент товаров и их количество, скорее всего, зависит от размера магазина. Период данных для большинства магазинов - 352 дня: от 01.08.2022 (10 магазинов из 12) до 18.07.2023, и один магазин - 309 дней: до 23.06.2023 Период проведения промо-акций - в среднем от 299 до 468 в день у 6 магазинов, еще 3 магазина предположительно закрылись или недавно открылись, и по ним не корректо оценивать. У 3 магазинов количество проводимых акций колеблется от 8 до 131.  


Далее были выялены и удалены строки с нулевым числом проданных товаров и ненулевыми продажами в рублях, а также с нулевыми продажами в рублях и ненулевым количеством проданных товаров. После этого были объединены данные по продажам с данными по товарной иерархии и по магазинам.

Были созданы новые столбцы:

 - total_sales_in_units - общие продажи в штуках, включая промо;
 - otal_sales_in_rub- общие продажи в рублях, включая промо;
 - price_mean - средняя цена товара;
 - promo_share - доля продаж по промо.
Были удалены ненужные столбцы: st_is_active, pr_sales_type_id, pr_sales_in_units, pr_promo_sales_in_units, pr_sales_in_rub, pr_promo_sales_in_rub, price, total_sales_in_rub_promo.

**На этапе EDA** было выявлено, что каждом столбце есть аномально большие и маленькие значения, а также отрицательные значения:

Отрицательные значения оказались возвратами в столбцах количества товара и суммы продаж. Они были оставлены.
В столбце price_mean оказались значения inf, так как продажи в штуках были равны 0, а сумма продаж не была равна 0. Такие строки были удалены.
48 товаров(17 категорий и 33 подкатегории) оказались с ценой ниже 5 рублей в 500 строках данных. Было решено сделать детекцию аномалий по цене в будущем.

Выбросы и пропуски были обработаны с помощью библиотеки ETNA, пропуски были заполнены на основании тех данных, которые у нас есть. Можно так же посчитать для каждого временного ряда сколько наблюдений присутствует в исторических данных и выделить это в отдельный признак.

Далее был проведен анализ временных рядов. Можно отметить сезонность, равную 1 неделе (7 дням). Для модели прогноза был сделан признак день недели с тригонометрическим преобразованием гиперпараметры max_lag, rolling_mean_size равными 7 дням (неделя). Было установлено при помощи критерия Дики-Фуллера, что временной ряд является стационарным.

Были проанализированы продажи товаров по месяцам. Ожидаемо, что максимальные продажи были в декабре. Данные за июль 2023 неполные, поэтому видна сильная разница с июнем. Так как видны колебания продаж по месяцам - стоит добавить признак, соответствующий месяцу.

После этого исследовались сгруппированные данные.

При анализе сумм продаж по магазином оказалось, что из 8 магазинов основные продажи приходятся на 6 магазинов - это около 98% оборота. Сосредоточимся на корректном предсказании продаж именно по этим магазинам.

Сгруппированные данные по группам товаров выявили топ-5 групп:

 - aab3238922bcc25a6f606eb525ffdc56
 - 6512bd43d9caa6e02c990b0a82652dca
 - c74d97b01eae257e44aa9d5bade97baf
 - c51ce410c124a10e0db5e4b97fc2af39
 - c20ad4d76fe97759aa27a0c99bff6710  

Самая продаваемая группа товаров aab3238922bcc25a6f606eb525ffdc56 включает в себя 9 категорий и 44 подкатегории, а также 695 товаров, что составляет 34% всех товаров, поэтому логично, что данная группа продается лучше остальных. Продажи около 160 млн. руб. Вторая по продаваемости группа 6512bd43d9caa6e02c990b0a82652dca отстает по выручке от первой на 18%, но при этом в ней в три раза меньше товаров. Продажи 130 млн. руб. Корректное предсказание первых двух групп товаров дает нам 56% от общих продаж, первых трех групп - 75%, четырех - 89%, пяти - 97%.

Далее были сгруппированы топ-5 групп с категориями. Выяснилось, что топ-5 групп имеют все 43 категории, самых прибыльных из них - 20. Анализ продаж по категориям показывает, что для каждой группы основную долю продаж приносит только несколько категорий. Например, в первой по продажам группе 10 категорий, из них первые пять обеспечивают 96% продаж. Для второй группы из 9 категорий первые 4 категории дают 99% продаж.

Далее были сгруппированы топ-5 групп с топ-20 категориями. Было выявлено, что количество уникальных субкатегорий - 165, а количество самых прибыльных субкатегорий - 71, по которым продажи внутри каждой их групп максимальны.

Эту информацию можно попробовать учитывать при принятии решения о том какие модели на каких частях данных обучать. Так как нам важно точнее предсказать продукты, которые приносят максимальные продажи.

**На этапе  Feature Engineering** было сгенерировано множество признаков различными методами.

Библиотека TSFresh. Выбрали настройку EfficientParameters. Это дало 777 признаков.

Кластеризация временных рядов. Отбор количества признаков был сделан через elbow method и Silhouette. Получилось 15 кластеров. 
Их распределение получилось довольно равномерно на обработанных данных, в отличие от сырых. 
Временные ряды в пределах одного кластера демонстрируют схожее поведение.
Сложности оказались в нехватке времени для проверки этого метода в модели.

Предсказания Prophet. Это дало нам 16 признаков, которые можно использовать для дальнейшего предсказания в качестве признаков для бустинговых моделей.
Это внесли в модель, так как метрика улучшилась после добавления признаков из Prophet.

**На этапе подбора моделей** было написано 3 класса:

Было написано 3 класса:

- Preprocessor - класс для предобработки данных перед использованием моделей. Методы класса осуществляют базовую предобработку данных, включающую в себя загрузку данных из файлов, удаление ненужных признаков, удаление наблюдений, не соответствующих ТЗ, создание новых признаков вроде флага, является ли дата праздничным днем. На данном этапе возможно добавить информацию о погоде, но так как заказчик не предоставил данных о геолокации магазинов, это не представляется возможным.

После предварительной обработки осуществляется комбинированное заполнение пропусков. Для каждого товара в разрезе магазинов пропуски временного ряда заполнены с использованием библиотеки etna. Для значений вне этого диапазона, но в рамках датафрейма оставшиеся пропуски заполнены нулями.
Добавлены признаки средней цены для товара за всю историю в имеющихся данных и средней цены за последние 14 дней.
Добавлены дополнительные признаки, полученные с помощью модели FBProphet.

- Optimizer - класс для подбора гиперпараметров моделей регрессоров Catboost, XGBoost, LightGBM.
Реализован механизм разбиение датасета на фолды(мы использовали 3 фолда) для осуществления кросс-валидации. На основании данного разбиения реализован механизм подбора гиперпараметров для моделей градиентного бустинга в кросс-валидации. Потенциальные утечки данных устранены методами создания future датафрейма для предсказаний на каждом фолде.

- Predictor - класс осуществляет предсказания на основе всего датафрейма в будущее. Данный класс берёт лучшую модель по итогам кросс-валидации и осуществляет прогнозирование временных рядов с её помощью.

На основе трёх классов был создан собственный пайплайн, который принимает датасет и автоматически осуществляет все стадии работы путём последовательного применения всех классов и их методов.

В работе были проверены следующие бустинги:

- XGBRegressor;
- LGBMRegressor;
- CatBoostRegressor.

Метрики всех моделей получались очень близкими, поэтому было решено окончательно выбрать 1 модель по быстродействию - _ LGBMRegressor_.

**Получившиеся результаты:**
1. Для одного магазина и одного товара

| Model  | Metric WAPE | params |
|-------|--------|--------|
| XGBRegressor | 0.34 | {'learning_rate': 0.051663534589206285, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.8057802307388677, 'colsample_bytree': 0.6688426091682363} Lags: 21 |
| LGBMRegressor | 0.35 | {'learning_rate': 0.005355136958142169, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.48266399056341697, 'colsample_bytree': 0.7186521778796882} Lags: 35 |
| CatBoostRegressor | 0.35 | {'learning_rate': 0.005995893556028684, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.5502953132612949, 'colsample_bytree': 0.30721765510860755} Lags: 21 |

2. Для моделей, обученных  на всех данных

| Model  | Metric WAPE | params |
|-------|--------|--------|
| LGBMRegressor | 0.74 |  {'learning_rate': 0.006220744752430538, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.6227066208927512, 'colsample_bytree': 0.10655018932206506} Lags: 21 |

Топ - 5 признаков для модели по графику feature importances:
 - price - цена
 - price_mean
 - day_of_week_cos
 - day_of_week_sin
 - day

shap опирается на те же признаки немного в другой последовательности.

На данный момент лучшие фичи - временные признаки (день недели) и признаки цены и средней цены. 

Поле для экспериментов:

1. Добавить признаки библиотеки TSFresh.
2. На данный момент лучшую метрику показывает стратегия когда обучается одна модель на каждую уникальную пару магазин - товар. Однако это не совсем эффективный подход. Мы планировали осуществить серию экспериментов при которых данные для обучения моделей разбивались бы по магазинам и товарам с учетом понимания того какие товары, группы товаров или магазины важно предсказывать точнее так как они вносят наибольший вклад в общую сумму продаж, а какие из товаров, групп, магазинов можно объединить и прогнозировать спрос для них одной моделью. 
3. Так как кластеризация показала, что внутри полученных кластеров временные ряды демонстрируют схожее поведение - это так же можно использовать для проведения экспериментов с целью получения разумного баланса между метрикой и количеством обучаемых моделей.